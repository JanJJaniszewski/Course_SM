---
title: "krr"
author: "Group3"
date: "11/17/2021"
output:
  html_document:
    df_print: paged
header-includes:
- \usepackage[utf8]{inputenc}
- \usepackage{natbib}
- \bibliographystyle{abbrvnat}
---
\section{Introduction}

\section{Data}
To measure the operational efficiency of six airlines in the United States between 1970-1984 in our sample, we measure the revenue per passenger-mile flown. Earlier research has shown (REF) that key performance indicators total cost of the airline, the fuel price, and the load factor* are important measures to predict operational efficiency. Further, we control for the year of observation, however, we do not use individual dummies for each year, but a continuous variable. This is important as when the model is used to predict a year forward this is not possible with a dummy for the year we are attempting to predict. Finally, a dummy is included for each of the 6 airlines.\\
We use the data from Greene (2003), which has annual observations on 6 airlines on the described variables, 15 observations per airline in total, summary statistics are included in Table XXX. We scale all variables except the airline dummies to create z-scores; $(X â€“ \mu) / \sigma$  where $\mu$ and $\sigma$ are the mean and the standard deviation respectively. This is essential, as our approach is relying on distance between observations, by scaling we ensure equal importance for each variable.

```{r, echo=FALSE}
library(Hmisc)
load('Airline.Rdata')
df <- Airline

Hmisc

```


\section{Method}

\section{Results}


```{r  qq-test, fig.cap = "QQ-plot of the best KRR model predictions of costs vs. real values of costs for the final holdout data and line indicating perfect prediction location"}
t1 <- tibble(real_costs = y_test, predicted_costs = predictions)
ggplot(data=t1, aes(x=real_costs, y=predicted_costs)) + geom_point() + 
  geom_abline(intercept = 0, slope = 1, size = 0.5)
```

```{r  qq-cv, fig.cap = "QQ-plot of the best KRR model predictions of costs vs. real values of costs for the training data  and line indicating perfect prediction location"}
t1 <- tibble(real_costs = y_train, predicted_costs = predictions_train)
ggplot(data=t1, aes(x=real_costs, y=predicted_costs)) + geom_point() + 
  geom_abline(intercept = 0, slope = 1, size = 0.5)
```
```{r  linearcomparison, fig.cap = "KRR with linear kernel: Impact of the choice of lambda parameter on the MSE"}
crossv_sub <- crossv_output %>% filter(kernel == 'linear')
crossv_sub %>%
  mutate(lambda=log(lambda)) %>%
  group_by(lambda) %>% summarise(mse = mean(mse)) %>%
  ggplot(data=., aes(lambda, mse)) + 
  labs(x ="Log(Lambda)", y = "MSE") +
  geom_line()
```
```{r  rbfcomparison, fig.cap = "KRR with RBF kernel: MSE for different combinations of lambda and gamma"}
crossv_sub <- crossv_output %>% filter(kernel == 'RBF')
crossv_sub %>%
  mutate(hyperparameter = log(hyperparameter), lambda=log(lambda)) %>%
  ggplot(data=., aes(lambda, hyperparameter, fill= mse)) + 
  geom_tile() + 
  labs(x ="Log(Lambda)", y = "Log(Gamma)") +
  scale_fill_gradient(low = "darkgreen", high = "darkred")
```
```{r  inhomogeneous, fig.cap = "KRR with inhomogeneous kernel: MSE for different combinations of lambda and d"}
crossv_sub <- crossv_output %>% filter(kernel == 'inhomogeneous')
crossv_sub %>%
  mutate(hyperparameter = log(hyperparameter), lambda=log(lambda)) %>%
  ggplot(data=., aes(lambda, hyperparameter, fill= mse)) + 
  geom_tile() + 
  labs(x ="Log(Lambda)", y = "Log(d)") +
  scale_fill_gradient(low = "darkgreen", high = "darkred")
```

```{r  polyspline, fig.cap = "KRR with polynomial spline kernel: MSE for different combinations of lambda and r"}
crossv_sub <- crossv_output %>% filter(kernel == 'polyspline')
crossv_sub %>%
  mutate(hyperparameter = log(hyperparameter), lambda=log(lambda)) %>%
  ggplot(data=., aes(lambda, hyperparameter, fill= mse)) + 
  geom_tile() + 
  labs(x ="Log(Lambda)", y = "Log(R)") +
  scale_fill_gradient(low = "darkgreen", high = "darkred")
```

\section{Discussion}