---
title: "Predicting Operational Efficiency Using Cost Metrics in the Airline Industry"
author: "Group 3"
date: "18/11/2021"
output:
  html_document:
    df_print: paged
  pdf_document:
    latex_engine: xelatex
header-includes:
  - \usepackage{graphicx} 
  - \usepackage{float}       
  - \usepackage[ngerman]{babel} 
  - \usepackage{fancyhdr}
  - \usepackage{hyperref}
  - \pagenumbering{gobble}
  - \usepackage{booktabs}
  - \usepackage{natbib}  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source('./krr.R')

```

## Introduction

The system-wide global commercial airlines constitute a high-cost industry, with expenses of approximately 795 and 484 billion dollars globally in 2019 and 2020 (Iata, 2021). This is why the revenue margins are highly dependent on its operational efficiency, the metric that measures the efficiency of profit earned as a function of operating costs. An industry is operationally efficient if conditions exist that allow participants to execute transactions and receive services at a price that equates fairly to the actual costs required to provide them. Airlines can become more efficient through improvement in a variety of areas, from which aircraft and crew utilization, payload, and fuel consumption are key. In this way, resources and the efficient use of such resources have a key impact in predicting profitability in airline firms. 

The aim of this research is thus to predict operational efficiency in the airline industry by using key cost metrics. The research uses kernels as a method that allows for non-linearity, making the predictions more flexible although probably at the cost of bias. The predictions are relevant for both managers and stakeholders. On the one hand, they are key for managers to choose relevant strategies for the consolidation of the firms. On the other hand, the information allows for a better understanding of the predictors of financial development in the industry and provides more stable financial decisions. 

The paper is structured as follows. In section 2, the airline data used for prediction. In section 3, the choice of kernel functions as a method is justified and outlined. In section 3, the results.... Finally, it is concluded that... 
DISCUSSION PERSON, PLEASE ADD TWO RELEVANT SENTENCES HERE (ONE FOR RESULTS AND ONE FOR DISCUSSION) TO SUMMARIZE THE SECTIONS.


## Data

The dataset, obtained through Greene (2003), contains empirical evidence relating to the cost of the airline industry. It consists of a panel of 6 observations of Airlines in the United States over 15 years- from 1970 to 1984, yielding 90 observations in total. Data was collected for all six firms in the same fifteen years. 

Each observation consists of six variables relating to cost:  airline, year,  pf (fuel price), lf (load factor, which is the average capacity utilization of the fleet), output (revenue, measured in revenue per passenger-mile flown), and cost (total cost per $1,000). Fuel price, load factor, output, and cost are numerical variables. Airline is treated as an indicator variable that classifies each airline in a distinct category. We control for the year of observation; however, this is done by using a continuous variable instead of individual dummies, as this allows the prediction of future years. All summary statistics are shown in Table 1.

All observations are used for the study. All variables except the airline indicator variable are scaled to create z-scores $(X â€“ \mu) / \sigma$  where $\mu$ and $\sigma$ represent the mean and the standard deviation, respectively. Scaling ensures that the variables are comparable, as our approach relies on the distance between observations.


| Variable                                  | Count |  Mean |  SD  |   Mininimum |   25% |   50% |   75% |   Maximum |
|:------------------------------------------|------:|------:|-----:|------:|------:|------:|------:|------:|
| Revenue per Passenger-Mile Flown (in USD) |    90 |  0.54 | 0.53 |  0.04 |  0.14 |  0.31 |  0.95 |  1.94 |
| Total Cost (in million USD)               |    90 |  1123 | 1192 |    69 |   292 |   637 |  1346 |  4748 |
| Fuel Price                                |    90 |   472 |  330 |   104 |   130 |   357 |   850 |  1016 |
| Load Factor                               |    90 | 56.0% | 5.3% | 43.2% | 52.9% | 56.6% | 59.5% | 67.6% |
Table NUMBER: Summary Statistics (SD: Standard deviation)

## Methodology

In the aim to increase the flexibility of the model- to decrease variance at the cost of bias-, we move beyond the standard linear regression model and use basis expansion models, popular in machine learning for their ability to extract more predictive information from the data set. In particular, kernels are used as a method that allows the introduction of non-linear components to linear regression, thus maintaining the convenience and ability of linear methods to not overfit. Kernels do so by embedding the points of any space into a new space where linearity can be applied via a non-linear feature map. The embedding computation is not made directly but through the kernel functions, built to be equivalent to the scalar products of the transformation (Hastie, Tibshirani, Friedman, 2008). In the analysis, polynomial kernels, radial basis function kernels and polyharmonic spline kernels are used.

Specifically, let $X \rightarrow R$ be the space that contains the data points $x_1,...,x_n \in X$, and $\Phi: X \rightarrow R^d$ a feature map where $\Phi$ denotes the $n \times d$ matrix containing data points $\Phi(x_i)$ as rows. Then, a symmetric function $k: X \times X \rightarrow R^d$ is a kernel function if for all $n \geq 1$ in $X$ and $c_1,...,c_n \in R^d$, it holds that: 

\begin{equation} \label{eq:1}
\sum^n_{i,j = 1} c_ic_jk(x_i,x_j) \geq 0.
\end{equation}

The kernel function $k$ must be a symmetric and positive semi-definite function. The kernel function has a corresponding kernel matrix $k$ with entries $k_{ij}=k(x_i,x_j)$. By definition, a valid kernel matrix coincides with matrix $\Phi \cdot \Phi^T \in R^d$, having entries:

\begin{equation} \label{eq:2}
k_{ij} =  \space <\Phi(x_i) , \Phi(x_j)> \space = \Phi(x_i) \Phi(x_j)^T \text{ for all } x,y \text{ in } X.
\end{equation}

Given $X$ and $k$, $\mathcal{H}$ denotes a Hilbert space with scalar product $< \cdot , \cdot >_{\mathcal{H}}$. The Reproducing Kernel Hilbert Space (RKHS) is the vector space $\mathcal{H}$ in which equality (2) holds. 

The aim of the kernel method is to solve the general regression problem as a problem of finding the function $f$ that minimizes:

\begin{equation} \label{eq:3}
\mathop{min H[f]}_{f \in \mathcal{H}} = \frac{1}{n} \sum^n_{i=1} (y_i - f(x_i)) + \lambda ||f||^2_K,
\end{equation}

where $||f||^2_K$ is a norm in the RKFH $\mathcal{H}$ defined by the positive definite function $K$, $n$ is the number of data points and $\lambda$ is the regularization parameter (Evgeniou, Pontil, & Poggio, 1999, p. 2). The term $\lambda ||f||^2_K$ is built as a quadratic penalty factor based on the Ridge regression, which imposes the penalty on the size of the regression coefficients. This reduces the high dimensionality and high correlation of the regressors, that are problematic for conventional techniques such as the Ordinary Least Square optimization. 

The introduction of kernels to the optimization problem (3) allows us to solve it purely in terms of kernel functions:

\begin{equation} \label{eq:4}
\mathop{maximize}_{\alpha \in R^n} \sum^n_{i=i} \alpha_i - \frac{1}{2} \sum^n_{i,j=1} \alpha_i \alpha_j y_i y_j K(x_i,x_j), \\
\end{equation}

subject to: 
$$
0 \leq \alpha_i \leq \frac{c}{n} \space \space  \forall_i=1,...,n ,
$$
$$
\sum^n_{i=i} \alpha_i Y_i = 0.
$$

The solution to the optimization problem can be found with the function $k$, ($X \times X$) with $k(x_i,y_i)=<\Phi(x_i),\Phi(x_j)>$.

Kernels not only allow us to find the minimization functional, but are able to do so in a finite-dimensional subspace even though its respective RKHS is an infinite dimensional vector space (Vapnik, 1995). This is because $\mathcal{H}$ can be subdivided in $\mathcal{H}_{data}$, the $span$ {$k_{xi},...,k_{xn}$}, and $\mathcal{H}_{comp}$, its orthogonal complement. Each vector $w \in \mathcal{H}$ is $w = w_{data} + w_{comp}$. The prediction of all functions with the same $w_{data}$ agree on all training points, and as such is not affected by $w_{comp}$. As such, the norm $w$ is smallest if $w_{comp}=0$, thus always being an optimal solution. In this way, kernels allows us to combat the computational difficulties related to very large number of parameters.  

In this analysis, three kernels are computed: the radial basis function (RBF) kernel, the inhomogeneous polynomial kernel (IPK), and the polyhomogeneous spline kernel (PSK). Different kernels are used to compare their ability to predict the data. Their functions are:

\begin{equation} \label{eq:5}
\text{ RBF:   }k_{ii'}= e^{(-\gamma||x_i-x_i'||^2)}\text{, with hyperparameter}  \gamma>0.
\end{equation}

\begin{equation} \label{eq:6}
\text{ IPK:   }k_{ii'} = (1 + x_i^Tx_i')^d, \text{ with fixed degree} d>0. 
\end{equation}

\begin{equation} \label{eq:7}
\text{PSK:\\}k_{ii'} = r^d, \text{ if } d=1,3,5,..., 
\end{equation}
$$
k_{ii'} = r^dln(r), \text{ if } d=2,4,6,...\text{   .}
$$
Intuitively, RDB captures the similarity at point $y$ to a fixed point $x$ as a function of their distance. IPK considers combinations of the features of input samples (interaction features) to determine similarity. PSK ... ADD ONE SENTENCE TO DESCRIBE INTUITIVELY WHAT IT DOES.

In both the cross-validation and the test set our predictions will be tested using the mean-squared prediction error ($MSPE$). $MSPE= \frac{1}{n}\sum^{n+q}_{i=n}(y_{i} - \hat{y}_{i})^{2}$, where $y_{i}$ is an output variable, and $\hat{y}_{i}$ is the predicted value over that period. In this example, $n$ is the number of observations in the training set, $q$ the number of observations in the test set. In each case a lower MSPE will be considered better predictive performance. 


## Results
\section{Results}
Analysis of the data and models used to predict airlines costs reveals that the airline cost predictions made by our best model are indeed significantly
better than random ($R^2=0.96$). An analysis of the QQ-plot for our cross-validation dataset as well as the test set reveals that all predictions of our model are close to their real values (i.e. there are no prediction outliers; see Figures \ref{QQ_train} and \ref{QQ_test}).

```{r  QQ_test, fig.cap = "QQ-plot of the best KRR model predictions of costs vs. real values of costs for the final holdout data and line indicating perfect prediction location"}
t1 <- tibble(real_costs = y_test, predicted_costs = predictions)
ggplot(data=t1, aes(x=real_costs, y=predicted_costs)) + geom_point() + 
  geom_abline(intercept = 0, slope = 1, size = 0.5)
```

```{r  QQ_train, fig.cap = "QQ-plot of the best KRR model predictions of costs vs. real values of costs for the training data  and line indicating perfect prediction location"}
t1 <- tibble(real_costs = y_train, predicted_costs = predictions_train)
ggplot(data=t1, aes(x=real_costs, y=predicted_costs)) + geom_point() + 
  geom_abline(intercept = 0, slope = 1, size = 0.5)
```

\subsection{Cross-validation and Out-of-sample Fit Estimates}

The data at hand includes a time-series component. In order to test the models against a real world scenario, it was split according to that nature: the test data comprehends the last 3 years of observations and the rest is used for training purposes. The train data is also used to obtain the hyperparameter combinations that optimizes the prediction performance of the models: a gridsearch was done using 20-fold cross-validation (the partitions splits are grouped by year, as in the train/data set split), and the out-of-sample performance was approximated by the mean 'mean squared error' obtained at each iteration.

The mse estimate on the test set is also obtained for each kernel, at the optimal hyperparameter mix.

\subsection{Cross-validation results}
To arrive at this conclusion, we analyzed different parameters for the different kernels described in the method section as well as compared them to an OLS regression and the package built by \citet{groenemodel}.

For all models, we used the same values for $\lambda$, namely 0.001, 0.01, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50 and 100.

\subsection{KRR with Linear Kernel}
The analysis of the KRR with linear kernel reveals that the best predictions of the model are provided given lower values for $\lambda$, with the result optimized at $\lambda = 0.1$ ($MSE_{CV} = 0.0026, MSE_{test} = 0.008$; for more information, see Figure \ref{linearcomparison}). 

```{r  linearcomparison, fig.cap = "KRR with linear kernel: Impact of the choice of lambda parameter on the MSE"}
crossv_sub <- crossv_output %>% filter(kernel == 'linear')
crossv_sub %>%
  mutate(lambda=log(lambda)) %>%
  group_by(lambda) %>% summarise(mse = mean(mse)) %>%
  ggplot(data=., aes(lambda, mse)) + 
  labs(x ="Log(Lambda)", y = "MSE") +
  geom_line()
```

\subsection{KRR with RBF Kernel}
For the KRR with RBF kernel, $\gamma$ was set to values between 0.01 and 10 ($\gamma \in [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 2, 4, 5, 10]$. The lowest measured MSE was achieved using $\lambda = 0.001$ and $\gamma = 0.05$ ($MSE_{CV} = 0.00048, MSE_{test} = 0.0026$; for more information, see Figure \ref{rbfcomparison}).

```{r  rbfcomparison, fig.cap = "KRR with RBF kernel: MSE for different combinations of lambda and gamma"}
crossv_sub <- crossv_output %>% filter(kernel == 'RBF')
crossv_sub %>%
  mutate(hyperparameter = log(hyperparameter), lambda=log(lambda)) %>%
  ggplot(data=., aes(lambda, hyperparameter, fill= mse)) + 
  geom_tile() + 
  labs(x ="Log(Lambda)", y = "Log(Gamma)") +
  scale_fill_gradient(low = "darkgreen", high = "darkred")
```

\subsection{KRR with Nonhomogeneous Kernel}
To optimize the hyperparameters of the nonhomogeneus kerner, we performed a grid search with the values 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 for the degree of the transformation. We obtained the minimum mse with the combination degree equals to 2 and lambda equals to 1.

The mean mse for the out-of-sample predictions under this model configuration is 0.00093 in the cross-validation and 0.009 in the test set (for more information, see Figure \ref{inhomogeneouscomparison})


```{r  inhomogeneouscomparison, fig.cap = "KRR with inhomogeneous kernel: MSE for different combinations of lambda and d"}
crossv_sub <- crossv_output %>% filter(kernel == 'inhomogeneous')
crossv_sub %>%
  mutate(hyperparameter = log(hyperparameter), lambda=log(lambda)) %>%
  ggplot(data=., aes(lambda, hyperparameter, fill= mse)) + 
  geom_tile() + 
  labs(x ="Log(Lambda)", y = "Log(d)") +
  scale_fill_gradient(low = "darkgreen", high = "darkred")
```

\subsection{KRR with polyspline kernel}
For the polyspline kernel, we defined the canditates for the hyperparameter degree as 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, and obtained the same combination as in the nonhomogeneous kernel: a degree of 2 and a lambda of 1. 

The resulting mse of the out-of-sample predictions is slightly higher than for the Nonhomogeneous kernel, at 0.0012 in the crossvalidation and 0.152 in the test set (for more information, see Figure \ref{polysplinecomparison}).


```{r  polysplinecomparison, fig.cap = "KRR with polynomial spline kernel: MSE for different combinations of lambda and r"}
crossv_sub <- crossv_output %>% filter(kernel == 'polyspline')
crossv_sub %>%
  mutate(hyperparameter = log(hyperparameter), lambda=log(lambda)) %>%
  ggplot(data=., aes(lambda, hyperparameter, fill= mse)) + 
  geom_tile() + 
  labs(x ="Log(Lambda)", y = "Log(R)") +
  scale_fill_gradient(low = "darkgreen", high = "darkred")
```

\subsection{Bench-Marking}

The kernel ridge regression was compared with the linear regression model and to the implementation available in the dsmle package.

The results indicate a good performance of the proprietary implementation: the linear regression showed a MSE of 0.572,
while the proprietary kernel ridge regression with the optimal combination of kernel and hyperparameters (RBF; parameters set at $\gamma = 0.05$ and $\lambda = 0.001$) predicts with $MSE = 0.0026$. The dsmle implementation (using the same parameters and kernel) is 0.30. As further comparison, the combination 'inhomogeneous kernel', degree of 2 and lambda equals to 1 yields a MSE of 0.01729 for the proprietary implementation and 0.0070 for the MSE of the dsmle package. 



## Discussion


## References
Evgeniou, T., Pontil, M., & Poggio, T. (1999). Regularization Networks and Support Vector Machines. Advances In Computational Mathematics, 13, 1-50. Retrieved 22 November 2021, from.

Greene, W.H. (2003) Econometric Analysis, Prentice Hall

Hastie, T., Tibshirani, R., & Friedman, J. (2008). The Elements of Statistical Learning: Data Mining, Inference, and Prediction (2nd ed.). Springer.

IATA Economics. (2021). Industry Statistics. IATA.

Vapnik, V. (1995). The Nature of Statistical Learning Theory (2nd ed.). Springer.

## Appendix I: Code
